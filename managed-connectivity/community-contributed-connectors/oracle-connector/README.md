# Oracle Connector

This custom connector extracts metadata from Oracle databases for import into [BigQuery universal catalog](https://cloud.google.com/dataplex/docs/catalog-overview).

Custom connectors are part of the [Managed Connectivity framework](https://cloud.google.com/dataplex/docs/managed-connectivity-overview) and are responsible for the export of metadata from external systems into correctly formatted import files. Import of metadata files generated by this connector and building pipelines is covered below in the section [Import metadata into universal catalog](#import-metadata-into-universal-catalog).

See the documentation [Develop Custom Connectors](https://cloud.google.com/dataplex/docs/develop-custom-connector) for more information about custom connectors for universal catalog.

### Target objects and schemas:

Metadata for the following database objects is extracted by the connector:
|Object|Metadata Extracted|
|---------|------------|
|Tables|Table name, column names, column data types|
|Views|View name, column names, column data types|

### Parameters
The Oracle connector accepts the following parameters:
|Parameter|Description|Default Value|Required/Optional|
|---------|------------|----|-------------|
|target_project_id|Google Cloud Project ID. Used in the generated metadata and defines the scope metadata will be imported into||REQUIRED|
|target_location_id|Google Cloud Region ID, or 'global'. Used in the generated metadata and indicates the region Entries will be associated with||REQUIRED|
|target_entry_group_id|Entry Group ID which the Entries will be associated with||REQUIRED|
|host|Oracle server to connect to||REQUIRED|
|port|Oracle server port|1521|REQUIRED|
|service|Oracle service to connect to. Either **--service** or **--sid** required||REQUIRED|
|sid|Oracle SID (Service Identifier). Either **--service** or **--sid** required||REQUIRED|
|user|Oracle Username to connect with||REQUIRED|
|password_secret|GCP Secret Manager ID holding the password for the Oracle user. Format: projects/[PROJ]/secrets/[SECRET]||REQUIRED|
|local_output_only|Generate metadata file in local directory only, do not push to cloud storage|False|OPTIONAL|
|output_bucket|Cloud Storage bucket where the output file will be stored. Required if **--local_output_only** = False||REQUIRED|
|output_folder|Folder in the Cloud Storage bucket where the output metadata file will be stored. Required if **--local_output_only** = False||
|jar|Name (or full path to) JDBC jar file to use for connection|ojdbc11.jar|OPTIONAL|
|min_expected_entries|Minimum number of entries expected in generated metadata file. If less file is not uploaded to Cloud Storage|-1|OPTIONAL|

Note: **target_project_id**, **target_location_id** and **target_entry_group_id** are used as string values in generated metadata files only and do not need to match the project where the connector is being run.

### Prepare your database environment:

Best practice is to connect to the database using a dedicated user with the minimum privileges required to extract metadata. 

1. Create a user in the Oracle instance(s) which has the following privileges and roles: 
    * CONNECT and CREATE SESSION to <monitoring_user>
    * GRANT SELECT on DBA_OBJECTS to <monitoring_user>
    * SELECT on all schemas for which metadata needs to be extracted

2. Add the password for the user to the Secret Manager in your google cloud project and note the ID (format is: projects/[project-number]/secrets/[secret-name])

## Set up and run the connector

The metadata connector can be run directly from the command line by executing the main.py script.

#### Prerequisites

The following tools and libraries are required to run the connector.

* Python 3.x. [See here for installation instructions](https://cloud.google.com/python/docs/setup#installing_python)
* A python Virtual Environment. Follow the instructions [here](https://cloud.google.com/python/docs/setup#installing_and_using_virtualenv) to create and activate your virtual environment.
* Java Runtime Environment (JRE)
    ```bash
    sudo apt install default-jre
    ```
* Install PySpark
    ```bash
    pip3 install pyspark
    ```
* The user that runs the connector must be authenticated with a Google Cloud identity in order to access the APIs for Secret Manager and cloud storage. You can use [Application Default Credentials](https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login) to do this. If you are not running the connector in a Google Cloud managed environment then you will need to [install the Google Cloud SDK](https://cloud.google.com/sdk/docs/install). 

The authenticated user have the following roles in the project: 
* roles/secretmanager.secretAccessor
* roles/storage.objectUser


```bash
    gcloud auth application-default login
```
#### Set up
* Ensure you are in the root directory of the connector
    ```bash
    cd oracle-connector
    ```
* Download the **odjcb11.jar** file from [Oracle](https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html) and save it in the local directory.
    **Note** If you need to use a different version of the JDBC jar then add the **--jar** parameter to the command. eg.   --jar odjbc17.jar
* Install all remaining python dependencies 
    ```bash
    pip3 install -r requirements.txt
    ```

#### Run the connector

```shell 
python3 main.py \
--target_project_id the-gcp-project-id \
--target_location_id us-central1 \
--target_entry_group_id oracle \
--host the-oracle-server \
--port 1521 \
--user dataplexagent \
--password_secret projects/73819994526/secrets/dataplexagent_oracle \
--service XEPDB1 \
--output_bucket dataplex_connectivity_imports \
--output_folder oracle
```

### Connector Output:
The connector generates a metadata extract file in JSONL format as described [in the documentation](https://cloud.google.com/dataplex/docs/import-metadata#metadata-import-file) and stores the file locally in the 'output' directory. The connector also uploads the file to the Google Cloud Storage bucket and folder specified in the **--output_bucket** and **--output_folder** parameters unless **--local-output_only True** is used.

A sample output from the Postgres connector can be found in the [sample](sample/) directory.

## Import metadata into universal catalog

To manually import a metadata import file generated by this connector into universal catalog see the documentation [Import metadata using a custom pipeline](https://cloud.google.com/dataplex/docs/import-metadata#import-metadata) for instructions on calling the API and considerations when importing custom metadata.

To create an end-to-end pipeline which runs metadata extraction using the connector and then submits an Import API job to bring the generated file into universal catalog, see the section below:  [Create an end-to-end metadata extraction and import pipeline to universal catalog](#create-an-end-to-end-metadata-extraction-and-import-pipeline-to-universal-catalog)

## Build a container and run the connector with Dataproc Serverless

Building a Docker container allows the connector to be run from a variety of Google Cloud services including [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs).

### Build the container (one-time task)

1. Ensure [docker](https://docs.docker.com/engine/install/) is installed in your environment.
2. Edit [build_and_push_docker.sh](build_and_push_docker.sh) and set the PROJECT_ID AND REGION as appropriate. The container will be stored in the Artifact Registry using this information.
3. Ensure the user which runs the script is authenticated as an Google Cloud Identity which has the 
[Artifact Registry Writer](https://cloud.google.com/artifact-registry/docs/access-control#roles) IAM role for the Artifact Registry in your project.
4. Make the script executable and run:
    ```bash
    chmod a+x build_and_push_docker.sh
    ./build_and_push_docker.sh
    ``` 

    The process will build a container called **universal-catalog-oracle-pyspark** and store it in Artifact Registry. This can take up to 5 minutes.

### Run a metadata extraction job with Dataproc Serverless

#### Setup

Before you submit a job to Dataproc Serverless:

1. Create or choose a Cloud Storage bucket to be used as a working directory for Dataproc. Use the path for the **--deps-bucket** parameter below.
2. The service account given in **--service-account** below must have the IAM roles described [here](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source#required-roles) to successfully run Dataproc. You can use this [script](../common_scripts/grant_SA_dataproc_roles.sh) to grant the required roles to your service account.

Note:
* If **--service-account** is not provided then the default compute Service Account for the project will be assumed.

#### Submit a Dataproc Serverless job

Run the containerised metadata connector with the following command, substituting appropriate values for your environment and unique batch ID:

```shell
gcloud dataproc batches submit pyspark \
    --project=my-gcp-project-id \
    --region=us-central1 \
    --batch=0001 \
    --deps-bucket=dataplex-metadata-collection-bucket \  
    --container-image=us-central1-docker.pkg.dev/my-gcp-project-id/docker-repo/univerisal-catalog-oracle-pyspark:latest \
    --service-account=440199992669-compute@developer.gserviceaccount.com \
    --network=projects/gcp-project-id/global/networks/default \
    main.py \
--  --target_project_id my-gcp-project-id \
      --target_location_id us-central1	\
      --target_entry_group_id oracle \
      --host the-oracle-server \
      --port 1521 \
      --user dataplexagent \
      --password_secret projects/73819994526/secrets/dataplexagent_oracle \
      --service XEPDB1 \
      --output_bucket gcs_output_bucket_path \
      --output_folder oracle
```

Note:
* If you need to use a different version of the JDBC jar you can store it in a Cloud Storage bucket and give the bucket path **--jar** parameter (ie --jars=gs://path/to/jar/mssql-jdbc-other.jar)

See the [documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark) for more information about Dataproc Serverless pyspark jobs.

## Create an end-to-end metadata extraction and import pipeline to universal catalog

An end-to-end metadata extraction and import pipeline with monitoring can be created using [Workflows](https://cloud.google.com/workflows) and scheduled to run on a regular basis.

Follow the documentation here: [Import metadata from a custom source using Workflows](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source) and use [this yaml file](https://github.com/GoogleCloudPlatform/cloud-dataplex/blob/main/managed-connectivity/cloud-workflows/byo-connector/templates/byo-connector.yaml) as a template.

Sample input parameters for an import job with Google Workflows can be found in the [workflows](workflows) directory to 
