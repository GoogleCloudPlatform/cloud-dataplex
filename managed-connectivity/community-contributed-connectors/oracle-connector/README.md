# Oracle Connector

This custom connector extracts metadata from Oracle databases for import into [Dataplex Universal Catalog](https://cloud.google.com/dataplex/docs/introduction).

Custom connectors are part of the [Managed Connectivity framework](https://cloud.google.com/dataplex/docs/managed-connectivity-overview) and are responsible for the export of metadata from external systems into correctly formatted import files. Import of metadata files generated by this connector and building pipelines is covered below in the section [Import metadata into Dataplex Universal Catalog](#import-metadata-into-dataplex-universal-catalog). 
See [Develop Custom Connectors](https://cloud.google.com/dataplex/docs/develop-custom-connector) for more information.

This is not an officially supported Google product and is provided on an as-is basis, without warranty. This project is not eligible for the [Google Open Source Software Vulnerability Rewards Program](https://bughunters.google.com/open-source-security).

### Target objects and schemas:

Metadata for the following database objects is extracted by the connector
|Object|Metadata Extracted|
|---------|------------|
|Tables|Table name, column names, column data types, column NULL/NOT NULL, column default value|
|Views|View name, column names, column data types, column NULL/NOT NULL, column default value|

Metadata is not collected for objects in oracle system schemas. See the **get_db_schemas** function in [oracle_connector.py](src/oracle_connector.py) for a complete list.

### Supported Authentication Methods

The following authentication methods are supported for connecting to Oracle 
* Password

### Parameters
The connector takes the following parameters

|Parameter|Description|Default Value|Required/Optional|
|---------|------------|----|-------------|
|target_project_id|Google Cloud Project ID. Used in the generated metadata and defines the scope metadata will be imported into||REQUIRED|
|target_location_id|Google Cloud Region ID, or 'global'. Used in the generated metadata and indicates the region Entries will be associated with||REQUIRED|
|target_entry_group_id|Entry Group ID which the Entries will be associated with||REQUIRED|
|host|Oracle server to connect to||REQUIRED|
|port|Oracle server port||REQUIRED|
|service|Oracle service to connect to. Either **--service** or **--sid** required||REQUIRED|
|sid|Oracle SID (Service Identifier). Either **--service** or **--sid** required||REQUIRED|
|user|Oracle Username to connect with||REQUIRED|
|password_secret|ID in Secret Manager holding the password for the user. Format: projects/{PROJECT-ID}/secrets/{SECRET}||REQUIRED|
|local_output_only|Generate metadata import file in local directory only, do not push to Cloud Storage|False|OPTIONAL|
|output_bucket|Cloud Storage bucket where the output file will be stored. Required if **--local_output_only** = False||REQUIRED|
|output_folder|Folder in the Cloud Storage bucket where the output metadata import file will be stored. Required if **--local_output_only** = False||
|jar|Name (or full path to) JDBC jar file to use for connection|ojdbc11.jar|OPTIONAL|
|min_expected_entries|Minimum number of entries expected in generated metadata import fil. If less file is not uploaded to Cloud Storage|-1|OPTIONAL|

Note: **target_project_id**, **target_location_id** and **target_entry_group_id** are used as string values in the generated metadata import file only and do not need to match the project where the connector is being run. These three values define the job scope used when importing the metadata into the catalog, see [components of a metadata job](https://cloud.google.com/dataplex/docs/import-metadata#components) for details.

## Create a database user

Best practice is to create a dedicated database user for the connector with the minimum privileges required to extract metadata.

1. Create an Oracle user with the following privileges and roles:
    ```sql
    CREATE USER dataplex_connector IDENTIFIED BY {password};
    GRANT CREATE SESSION TO dataplex_connector;
    GRANT SELECT_CATALOG_ROLE TO dataplex_connector;
    ```

2. Add the password for the user to the Secret Manager in your Google Cloud project and note the ID (format is: projects/{project-number}/secrets/{secret-name})

## Install the connector

Follow these instructions to install and set up the connector to run directly from the command line.

### Prerequisites

The following components must be installed to run the connector:

* Python 3.x. Installation on Linux:
    ```
    sudo apt update
    sudo apt install python3 python3-dev python3-venv python3-pip
    ```
* Python Virtual Environment. Create the environment and activate:
    ```shell
    python3 -m venv env
    source env/bin/activate
    ```

    Also run ```source env/bin/activate``` each time before you use the connector in future.

* Java Runtime Environment (JRE)
    ```shell
    sudo apt install default-jre
    ```
* Install PySpark
    ```shell
    pip3 install pyspark
    ```

### Set-up
* Clone the connector repository
    ```bash
    git clone https://github.com/GoogleCloudPlatform/cloud-dataplex.git
    ```
* Ensure you are in the root directory of the oracle connector
    ```bash
    cd cloud-dataplex/managed-connectivity/community-contributed-connectors/oracle-connector
    ```
* Download the **ojdcb11.jar** file from [Oracle](https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html) and save it to the oracle-connector directory.

    Note if you need to use a different version of the JDBC jar, then add the **--jar** parameter to identify the jar file eg.   --jar ojdbc17.jar
* Install all remaining python dependencies 
    ```bash
    pip3 install -r requirements.txt
    ```

### Authentication and Authorization for the connector in Google Cloud
Before running the connector from the command line, ensure your session user is authenticated as a Google Cloud identity that has been granted read access to Secret Manager and read/write to Cloud Storage in the current project.  The user requires the following IAM roles in the project where the connector runs:

* roles/secretmanager.secretAccessor
* roles/storage.objectUser

You can use [Application Default Credentials](https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login) to ensure the session is authenticated

```bash
gcloud auth application-default login
```

Note: If you are not running the connector in a Google Cloud managed environment then you need to first install the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk)

## Run the connector from the command line

Run the following command from the root directory of the connector, substituting appropriate values and parameters for your environment as needed:

```shell 
python3 main.py \
--target_project_id gcp-project-id \
--target_location_id us-central1 \
--target_entry_group_id oracle \
--host the-oracle-server \
--port 1521 \
--user dataplexagent \
--password_secret projects/{gcp-project-number}/secrets/dataplexagent_oracle \
--service XEPDB1 \
--output_bucket dataplex_connectivity_imports \
--output_folder oracle
```

## Connector Output
The connector generates a metadata import file in JSONL format as described [in the documentation](https://cloud.google.com/dataplex/docs/import-metadata#metadata-import-file) and stores the file locally in the 'output' directory. The connector also uploads the file to the Google Cloud Storage bucket and folder specified in the **--output_bucket** and **--output_folder** parameters unless **--local-output_only True** is used.

A sample output from the Oracle connector can be found in the [sample](sample/) directory.

## Import metadata into Dataplex Universal Catalog

To manually import a metadata import file generated by this connector into Dataplex Universal Catalog see the documentation [Import metadata using a custom pipeline](https://cloud.google.com/dataplex/docs/import-metadata#import-metadata) for instructions on calling the API and considerations when importing custom metadata.

Note before importing metadata, the Entry Group and all Entry Types and Aspect Types found in the metadata import file must exist in the target project and location (either a Google Cloud region, or 'global'). This connector requires the following Entry Group, Entry Types and Aspect Types:

|Catalog Object|IDs required by connector|
|---------|-------------------|
Entry Group|Defined in **--target_entry_group_id** when metadata is extracted|
Entry Types|**oracle-instance**&nbsp;&nbsp;**oracle-database**&nbsp;&nbsp;**oracle-schema**&nbsp;&nbsp;**oracle-table**&nbsp;&nbsp;**oracle-view**|  
Aspect Types|**oracle-instance**&nbsp;&nbsp;**oracle-database**&nbsp;&nbsp;**oracle-schema**&nbsp;&nbsp;**oracle-table**&nbsp;&nbsp;**oracle-view**|  

See [manage entries and create custom sources](https://cloud.google.com/dataplex/docs/ingest-custom-sources) for instructions on creating Entry Groups, Entry Types, and Aspect Types.

To create an end-to-end pipeline which extracts metadata and starts an Import API job to bring the generated file into universal catalog, see section [Create an end-to-end metadata extraction and import pipeline to Dataplex Universal Catalog](#create-an-end-to-end-metadata-extraction-and-import-pipeline-to-dataplex-universal-catalog)

## Build a container and run the connector with Dataproc Serverless

Follow these instructions to build a Docker container which allows the connector to be run with [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs).

### Build the container (one-time task)

1. Ensure [docker](https://docs.docker.com/engine/install/) is installed in your environment.
2. Edit [build_and_push_docker.sh](build_and_push_docker.sh) and set the PROJECT_ID AND REGION as appropriate. The container will be stored in the Artifact Registry using this information.
3. Ensure the user which runs the script is authenticated as a Google Cloud Identity which has the 
[Artifact Registry Writer](https://cloud.google.com/artifact-registry/docs/access-control#roles) IAM role for the Artifact Registry in your project.
4. Make the script executable and run:
    ```bash
    chmod a+x build_and_push_docker.sh
    ./build_and_push_docker.sh
    ``` 

    The process will build a container called **catalog-oracle-pyspark** and store it in Artifact Registry. This can take up to 5 minutes.

### Run a metadata extraction job with Dataproc Serverless

#### Set-up

Before you submit a job to Dataproc Serverless:

1. Create or choose a Cloud Storage bucket to be used as a working directory for Dataproc. Use the path for the **--deps-bucket** parameter below.
2. The service account given in **--service-account** below must have the IAM roles described [here](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source#required-roles) to successfully run Dataproc. You can use this [script](../common_scripts/grant_SA_dataproc_roles.sh) to grant the required roles to your service account.

Note:
* If **--service-account** is not provided then the default Compute Service Account for the project will be assumed.

#### Submit a Dataproc Serverless job

* Ensure you are in the root directory of the connector
    ```bash
    cd oracle-connector
    ```

Run a Dataproc serverless job with the containerised metadata connector using the following command, substituting appropriate values for your environment and provide a unique batch ID in **--batch** :

```shell
gcloud dataproc batches submit pyspark \
    --project=gcp-project-id \
    --region=us-central1 \
    --batch=oracle-metadata-0001 \
    --deps-bucket=dataplex-metadata-collection-bucket \  
    --container-image=us-central1-docker.pkg.dev/gcp-project-id/docker-repo/catalog-oracle-pyspark:latest \
    --service-account=gcp-project-number-compute@developer.gserviceaccount.com \
    --jars=ojdbc11.jar \
    --network=default \
    main.py \
--  --target_project_id my-gcp-project-id \
      --target_location_id us-central1	\
      --target_entry_group_id oracle \
      --host the-oracle-server \
      --port 1521 \
      --user dataplexagent \
      --password_secret projects/gcp-project-number/secrets/dataplexagent_oracle \
      --service XEPDB1 \
      --output_bucket gcs_output_bucket_path \
      --output_folder oracle
```

Note:
* If you need to use a different version of the JDBC jar you can store it in a Cloud Storage bucket and give the bucket path **--jar** parameter (ie --jars=gs://path/to/jar/ojdbc17.jar)

See the [documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark) for more information about Dataproc Serverless pyspark jobs.

## Create an end-to-end metadata extraction and import pipeline to Dataplex Universal Catalog

An end-to-end metadata extraction and import pipeline with monitoring can be created using [Workflows](https://cloud.google.com/workflows) and scheduled to run on a regular basis.

Follow the documentation here: [Import metadata from a custom source using Workflows](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source) and use [this yaml file](https://github.com/GoogleCloudPlatform/cloud-dataplex/blob/main/managed-connectivity/cloud-workflows/byo-connector/templates/byo-connector.yaml) as a template.

An example of input parameters for creating an import job with Workflows can be found in the [workflows](workflows/) directory.